# encoding: utf-8
"""
çŸ¥è¯†åº“æœåŠ¡å±‚ï¼Œå°è£…çŸ¥è¯†åº“ç›¸å…³ä¸šåŠ¡é€»è¾‘ã€‚
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional
from pathlib import Path
from datetime import datetime
import json

from infrastructure.external.blog.loader import BlogDocumentLoader
from infrastructure.vector_store.chroma import VectorStore
from domain.knowledge_base.rag import RAGEngine
from shared.logger import log

class KnowledgeBaseService:
    """çŸ¥è¯†åº“æœåŠ¡ï¼Œæä¾›æ–‡æ¡£åŒæ­¥å’Œé—®ç­”åŠŸèƒ½ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“æœåŠ¡ã€‚"""
        self.document_loader = BlogDocumentLoader()
        self._rag_engine = None
        self._web_search_service = None
        # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        self.results_dir = project_root / 'data' / 'query_results'
        self.results_dir.mkdir(parents=True, exist_ok=True)

    @property
    def rag_engine(self) -> RAGEngine:
        """
        è·å–RAGå¼•æ“ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰ã€‚
        
        Returns:
            RAGå¼•æ“å®ä¾‹
            
        Raises:
            ImportError: å¦‚æœç¼ºå°‘å¿…è¦çš„ä¾èµ–
        """
        if self._rag_engine is None:
            try:
                self._rag_engine = RAGEngine()
            except ImportError as e:
                raise ImportError(
                    f"çŸ¥è¯†åº“åŠŸèƒ½ä¸å¯ç”¨ï¼ˆç¼ºå°‘ä¾èµ–ï¼‰: {e}\n"
                    "è¯·å®‰è£…ä¾èµ–: pip install sentence-transformers chromadb"
                ) from e
        return self._rag_engine

    def sync_blog_posts(self, incremental: bool = True) -> Dict[str, Any]:
        """
        åŒæ­¥åšå®¢æ–‡ç« ï¼ˆæ”¯æŒå¢é‡åŒæ­¥ï¼‰ã€‚

        Args:
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡åŒæ­¥ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            åŒæ­¥ç»“æœï¼ŒåŒ…å«åŒæ­¥çš„æ–‡æ¡£æ•°é‡å’ŒçŠ¶æ€
        """
        try:
            log.info(f"å¼€å§‹åŒæ­¥åšå®¢æ–‡ç«  (å¢é‡æ¨¡å¼: {incremental})")

            # åŠ è½½æ‰€æœ‰åšå®¢æ–‡ç« 
            documents = self.document_loader.load_all_posts()

            if not documents:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°æ–‡æ¡£",
                    "document_count": 0,
                    "new_count": 0,
                    "updated_count": 0,
                    "skipped_count": 0,
                }

            # å¢é‡åŒæ­¥ï¼šè·å–å·²æœ‰æ–‡æ¡£
            existing_docs = {}
            if incremental:
                # è·å–æ‰€æœ‰æ¥æºä¸º blog çš„æ–‡æ¡£
                try:
                    all_docs = self.rag_engine.vector_store._collection.get(
                        where={"source": "blog"}
                    )
                    # æŒ‰æ–‡æ¡£IDåˆ†ç»„ï¼ˆå»é™¤chunkåç¼€ï¼‰
                    for doc_id, metadata in zip(all_docs.get("ids", []), all_docs.get("metadatas", [])):
                        # chunk_idæ ¼å¼ï¼š{post_id}_chunk_{idx}ï¼Œæå–post_id
                        post_id = doc_id.split("_chunk_")[0]
                        if post_id not in existing_docs:
                            existing_docs[post_id] = metadata
                except Exception as e:
                    log.warning(f"è·å–å·²æœ‰æ–‡æ¡£å¤±è´¥: {e}")
                log.info(f"å‘é‡åº“ä¸­å·²æœ‰ {len(existing_docs)} ä¸ªåšå®¢æ–‡ç« ")

            # å‡†å¤‡æ–‡æ¡£æ•°æ®ï¼ˆåªåŒæ­¥æ–°å¢æˆ–æ›´æ–°çš„æ–‡æ¡£ï¼‰
            doc_data = []
            new_count = 0
            updated_count = 0
            skipped_count = 0
            current_doc_ids = set()

            for doc in documents:
                doc_id = doc["id"]
                current_doc_ids.add(doc_id)
                doc_date = doc["metadata"].get("date", 0)
                
                # å¢é‡åŒæ­¥ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                if incremental and doc_id in existing_docs:
                    existing_date = existing_docs[doc_id].get("date", 0)
                    # æ¯”è¾ƒæ—¥æœŸï¼ˆæ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼‰
                    if doc_date <= existing_date:
                        skipped_count += 1
                        log.debug(f"è·³è¿‡æœªæ›´æ–°çš„æ–‡ç« : {doc['metadata'].get('title', 'æœªçŸ¥')}")
                        continue
                    updated_count += 1
                    log.debug(f"æ–‡ç« å·²æ›´æ–°: {doc['metadata'].get('title', 'æœªçŸ¥')}")
                else:
                    new_count += 1

                doc_data.append({
                    "id": doc_id,
                    "content": doc["content"],
                    "metadata": {
                        **doc["metadata"],
                        "source": "blog",
                    },
                })

            # åˆ é™¤å·²ä¸å­˜åœ¨çš„æ–‡æ¡£ï¼ˆå¢é‡åŒæ­¥æ—¶ï¼‰
            deleted_count = 0
            if incremental and existing_docs:
                deleted_ids = set(existing_docs.keys()) - current_doc_ids
                if deleted_ids:
                    log.info(f"å‘ç° {len(deleted_ids)} ä¸ªå·²åˆ é™¤çš„æ–‡ç« ï¼Œå‡†å¤‡æ¸…ç†...")
                    for deleted_id in deleted_ids:
                        try:
                            # æŸ¥è¯¢è¯¥æ–‡ç« çš„æ‰€æœ‰chunk
                            all_docs = self.rag_engine.vector_store._collection.get(
                                where={"source": "blog"}
                            )
                            chunk_ids_to_delete = [
                                doc_id for doc_id in all_docs.get("ids", [])
                                if doc_id.startswith(f"{deleted_id}_chunk_") or doc_id == deleted_id
                            ]
                            if chunk_ids_to_delete:
                                self.rag_engine.vector_store.delete(ids=chunk_ids_to_delete)
                                deleted_count += 1
                                log.info(f"å·²åˆ é™¤æ–‡ç« : {deleted_id}")
                        except Exception as e:
                            log.warning(f"åˆ é™¤æ–‡ç« å¤±è´¥ {deleted_id}: {e}")

            # å¦‚æœæœ‰éœ€è¦åŒæ­¥çš„æ–‡æ¡£ï¼Œå…ˆåˆ é™¤æ—§ç‰ˆæœ¬å†ç´¢å¼•æ–°ç‰ˆæœ¬
            if doc_data:
                # å…ˆåˆ é™¤éœ€è¦æ›´æ–°çš„æ–‡æ¡£çš„æ—§ç‰ˆæœ¬
                if incremental:
                    ids_to_update = {doc["id"] for doc in doc_data}
                    for doc_id in ids_to_update:
                        try:
                            all_docs = self.rag_engine.vector_store._collection.get(
                                where={"source": "blog"}
                            )
                            chunk_ids_to_delete = [
                                chunk_id for chunk_id in all_docs.get("ids", [])
                                if chunk_id.startswith(f"{doc_id}_chunk_") or chunk_id == doc_id
                            ]
                            if chunk_ids_to_delete:
                                self.rag_engine.vector_store.delete(ids=chunk_ids_to_delete)
                        except Exception as e:
                            log.warning(f"åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥ {doc_id}: {e}")

                # ç´¢å¼•æ–‡æ¡£
                indexed_count = self.rag_engine.index_documents(doc_data)
            else:
                indexed_count = 0
                log.info("æ²¡æœ‰éœ€è¦åŒæ­¥çš„æ–‡ç« ")

            return {
                "success": True,
                "message": "åŒæ­¥æˆåŠŸ",
                "document_count": len(documents),
                "new_count": new_count,
                "updated_count": updated_count,
                "skipped_count": skipped_count,
                "deleted_count": deleted_count,
                "indexed_count": indexed_count,
            }

        except Exception as e:
            log.error(f"åŒæ­¥æ–‡æ¡£å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"åŒæ­¥å¤±è´¥: {str(e)}",
                "document_count": 0,
                "new_count": 0,
                "updated_count": 0,
                "skipped_count": 0,
                "deleted_count": 0,
            }

    def sync_all_spaces(self, incremental: bool = True) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´ã€‚

        Args:
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡åŒæ­¥ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            åŒæ­¥ç»“æœ
        """
        try:
            # è·å–æ‰€æœ‰çŸ¥è¯†åº“ç©ºé—´
            spaces = self.document_loader.load_wiki_spaces()

            total_documents = 0
            total_new = 0
            total_updated = 0
            total_skipped = 0
            total_deleted = 0
            success_count = 0
            failed_spaces = []

            for space in spaces:
                space_id = space.get("space_id", "")
                space_name = space.get("name", "æœªçŸ¥")

                if not space_id:
                    continue

                log.info(f"åŒæ­¥çŸ¥è¯†åº“ç©ºé—´: {space_name} ({space_id})")

                result = self.sync_documents_from_space(space_id, incremental=incremental)
                if result["success"]:
                    success_count += 1
                    total_documents += result["document_count"]
                    total_new += result.get("new_count", 0)
                    total_updated += result.get("updated_count", 0)
                    total_skipped += result.get("skipped_count", 0)
                    total_deleted += result.get("deleted_count", 0)
                else:
                    failed_spaces.append({
                        "space_id": space_id,
                        "name": space_name,
                        "error": result["message"],
                    })

            sync_mode = "å¢é‡" if incremental else "å…¨é‡"
            return {
                "success": True,
                "message": f"åŒæ­¥å®Œæˆï¼ˆ{sync_mode}æ¨¡å¼ï¼‰ï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {len(failed_spaces)} ä¸ª",
                "total_spaces": len(spaces),
                "success_count": success_count,
                "failed_count": len(failed_spaces),
                "total_documents": total_documents,
                "new_count": total_new,
                "updated_count": total_updated,
                "skipped_count": total_skipped,
                "deleted_count": total_deleted,
                "failed_spaces": failed_spaces,
            }

        except Exception as e:
            error_msg = str(e)
            log.error(f"åŒæ­¥æ‰€æœ‰çŸ¥è¯†åº“å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™é‡æ–°æŠ›å‡ºä»¥ä¾¿APIå±‚å¤„ç†
            is_auth_error = (
                "99991672" in error_msg or 
                "99991663" in error_msg or 
                "99991664" in error_msg or 
                "99991679" in error_msg or
                "æƒé™" in error_msg or 
                "Access denied" in error_msg or
                "unauthorized" in error_msg.lower() or
                "forbidden" in error_msg.lower()
            )
            if is_auth_error:
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©APIå±‚è¿”å›403
            
            return {
                "success": False,
                "message": f"åŒæ­¥å¤±è´¥: {error_msg}",
                "total_spaces": 0,
                "success_count": 0,
                "failed_count": 0,
                "total_documents": 0,
            }

    @property
    def web_search_service(self):
        """è·å–ç½‘ç»œæœç´¢æœåŠ¡ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._web_search_service is None:
            try:
                from infrastructure.external.web_search import WebSearchService
                self._web_search_service = WebSearchService()
            except Exception as e:
                log.warning(f"ç½‘ç»œæœç´¢æœåŠ¡ä¸å¯ç”¨: {e}")
                self._web_search_service = None
        return self._web_search_service

    def ask(self, question: str, space_id: Optional[str] = None, use_web_search: bool = False) -> Dict[str, Any]:
        """
        å›ç­”é—®é¢˜ã€‚
        
        ä½¿ç”¨å‘é‡æœç´¢æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°å‘é‡å­˜å‚¨è¿›è¡Œè¯­ä¹‰æœç´¢ï¼ˆéœ€è¦å…ˆåŒæ­¥æ–‡æ¡£ï¼‰

        Args:
            question: ç”¨æˆ·é—®é¢˜
            space_id: æŒ‡å®šæœç´¢çš„åšå®¢åˆ†ç±»ï¼Œå¦‚æœä¸æä¾›åˆ™æœç´¢æ‰€æœ‰æ–‡ç« 
            use_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢ï¼ˆé»˜è®¤Falseï¼‰ã€‚å½“çŸ¥è¯†åº“ç»“æœä¸ç†æƒ³æ—¶ï¼Œä¼šä½¿ç”¨ç½‘ç»œæœç´¢è¡¥å……

        Returns:
            ç­”æ¡ˆå’Œå¼•ç”¨æ¥æº
        """
        try:
            # æ£€æŸ¥å‘é‡å­˜å‚¨ä¸­æ˜¯å¦æœ‰æ–‡æ¡£
            collection_info = self.get_collection_info()
            has_local_docs = (
                collection_info.get("success") 
                and collection_info.get("info", {}).get("count", 0) > 0
            )
            
            if not has_local_docs:
                return {
                    "success": False,
                    "answer": "å‘é‡æ•°æ®åº“ä¸­è¿˜æ²¡æœ‰åšå®¢æ–‡ç« ï¼Œè¯·å…ˆåŒæ­¥åšå®¢æ–‡ç« ã€‚\n\næç¤ºï¼šåšå®¢æ–‡ç« ä¼šåœ¨ `hexo generate` æ—¶è‡ªåŠ¨åŒæ­¥ï¼Œæˆ–è¿è¡Œ `npm run sync-blog` æ‰‹åŠ¨åŒæ­¥ã€‚",
                    "sources": [],
                    "suggest_web_search": True,
                    "max_similarity": 0.0,
                }
            
            # ä½¿ç”¨å‘é‡æœç´¢æ¨¡å¼
            # æ³¨æ„ï¼šç›®å‰å‘é‡æœç´¢ä¸æ”¯æŒæŒ‰åˆ†ç±»è¿‡æ»¤ï¼Œspace_id å‚æ•°æš‚æ—¶å¿½ç•¥
            if space_id:
                log.warning(f"å‘é‡æœç´¢æš‚ä¸æ”¯æŒæŒ‰åˆ†ç±»è¿‡æ»¤ï¼Œå°†æœç´¢æ‰€æœ‰æ–‡ç« ï¼ˆå¿½ç•¥åˆ†ç±»: {space_id}ï¼‰")
            result = self.rag_engine.qa(question)
                
            # ä»RAGç»“æœä¸­è·å–ç›¸ä¼¼åº¦ä¿¡æ¯ï¼ˆRAGå¼•æ“å·²ç»è®¡ç®—å¥½äº†ï¼‰
            sources = result.get("sources", [])
            max_similarity = result.get("max_similarity", 0.0)  # ä½¿ç”¨RAGå¼•æ“è®¡ç®—çš„max_similarity
            avg_similarity = result.get("avg_similarity", 0.0)  # ä½¿ç”¨RAGå¼•æ“è®¡ç®—çš„avg_similarity
            
            # å¦‚æœæ²¡æœ‰ä»RAGç»“æœä¸­è·å–åˆ°ï¼Œåˆ™ä»sourcesè®¡ç®—
            if max_similarity == 0.0 and sources:
                max_similarity = max([s.get("similarity", 0) for s in sources])
            
            log.info(f"é—®ç­”ç»“æœ - æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}, å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}, æ¥æºæ•°: {len(sources)}")
            
            # æ„å»ºç”¨äºåˆ¤æ–­ç½‘ç»œæœç´¢çš„resultå­—å…¸ï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µï¼‰
            kb_result_for_search = {
                "success": len(sources) > 0,  # æœ‰æ¥æºå°±è®¤ä¸ºæˆåŠŸ
                "sources": sources,
                "answer": result.get("answer", ""),
                "max_similarity": max_similarity,
            }
                
                # åˆ¤æ–­æ˜¯å¦å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
            suggest_web_search = self._should_use_web_search(question, kb_result_for_search)
                
                # å¦‚æœå¯ç”¨äº†ç½‘ç»œæœç´¢ï¼Œä¸”çŸ¥è¯†åº“ç»“æœä¸ç†æƒ³ï¼Œå°è¯•ç½‘ç»œæœç´¢
                if use_web_search and suggest_web_search:
                    log.info("ğŸŒ çŸ¥è¯†åº“ç»“æœä¸ç†æƒ³ï¼Œå°è¯•ä½¿ç”¨ç½‘ç»œæœç´¢è¡¥å……...")
                web_result = self._search_web_and_merge(question, result)
                    return web_result
                
                # å¦‚æœæœªå¯ç”¨ç½‘ç»œæœç´¢ï¼Œä½†å»ºè®®ä½¿ç”¨ï¼Œåœ¨ç»“æœä¸­æ·»åŠ å»ºè®®ä¿¡æ¯
                if not use_web_search:
                result["suggest_web_search"] = suggest_web_search
                result["max_similarity"] = max_similarity
                    if suggest_web_search:
                        log.info(f"ğŸ’¡ å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}ï¼‰")
                
            # æ£€æŸ¥ç­”æ¡ˆè´¨é‡ï¼šå¦‚æœç­”æ¡ˆåŒ…å«å¦å®šæ€§è¡¨è¿°ä½†ç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œè®°å½•è­¦å‘Š
            answer = result.get("answer", "")
            negative_keywords = ["æ²¡æœ‰æ‰¾åˆ°", "æœªæ‰¾åˆ°", "ä¸ç›¸å…³", "æ— æ³•æ‰¾åˆ°", "æ²¡æœ‰ç›¸å…³ä¿¡æ¯"]
            has_negative = any(keyword in answer for keyword in negative_keywords)
            
            # å¦‚æœç›¸ä¼¼åº¦è¾ƒé«˜ï¼ˆ>=0.7ï¼‰ä½†ç­”æ¡ˆåŒ…å«å¦å®šæ€§è¡¨è¿°ï¼Œè®°å½•è­¦å‘Š
            if max_similarity >= 0.7 and has_negative:
                log.warning(f"ç­”æ¡ˆåŒ…å«å¦å®šæ€§è¡¨è¿°ï¼Œä½†æ–‡æ¡£ç›¸ä¼¼åº¦è¾ƒé«˜({max_similarity:.3f})ï¼Œå¯èƒ½å­˜åœ¨Promptç†è§£é—®é¢˜")
            
            return {
                "success": True,
                "answer": result["answer"],
                "sources": result["sources"],
                "suggest_web_search": result.get("suggest_web_search", False),
                "max_similarity": max_similarity,
                "avg_similarity": avg_similarity,
            }

        except Exception as e:
            log.error(f"å›ç­”é—®é¢˜å¤±è´¥: {e}")
            return {
                "success": False,
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "sources": [],
            }
    
    def get_wiki_spaces(self) -> Dict[str, Any]:
        """
        è·å–åšå®¢åˆ†ç±»åˆ—è¡¨ï¼ˆç”¨äºå…¼å®¹ get_wiki_spaces APIï¼‰ã€‚
        
        Returns:
            åˆ†ç±»åˆ—è¡¨
        """
        try:
            categories = self.document_loader.get_blog_categories()
            
            return {
                "success": True,
                "spaces": categories,
                "message": f"æ‰¾åˆ° {len(categories)} ä¸ªåšå®¢åˆ†ç±»",
            }
        except Exception as e:
            log.error(f"è·å–åšå®¢åˆ†ç±»åˆ—è¡¨å¤±è´¥: {e}")
                return {
                    "success": False,
                    "spaces": [],
                "message": f"è·å–åšå®¢åˆ†ç±»åˆ—è¡¨å¤±è´¥: {str(e)}",
            }
    
    def _save_query_result(self, question: str, step: str, data: Dict[str, Any], query_timestamp: Optional[str] = None):
        """ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # å¦‚æœæä¾›äº†query_timestampï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ç”Ÿæˆæ–°çš„
            if query_timestamp is None:
                query_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            filename = f"query_{query_timestamp}.json"
            filepath = self.results_dir / filename
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ æ•°æ®ï¼›å¦åˆ™åˆ›å»ºæ–°æ–‡ä»¶
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
            else:
                result_data = {
                    "question": question,
                    "timestamp": query_timestamp,
                    "steps": {}
                }
            
            result_data["steps"][step] = {
                "time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "data": data
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            log.info(f"ğŸ’¾ æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°: {filepath} (æ­¥éª¤: {step})")
            return query_timestamp  # è¿”å›æ—¶é—´æˆ³ï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        except Exception as e:
            log.warning(f"ä¿å­˜æŸ¥è¯¢ç»“æœå¤±è´¥: {e}")
            return None
    def _detect_question_type(self, question: str) -> Dict[str, Any]:
        """
        æ£€æµ‹é—®é¢˜ç±»å‹ï¼šæ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢ vs å†…å®¹é—®ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            {
                "type": "document_list" | "content_qa" | "mixed",
                "confidence": 0.0-1.0,
                "keywords": ["å…³é”®è¯åˆ—è¡¨"]
            }
        """
        question_lower = question.lower()
        
        # æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢çš„å…³é”®è¯æ¨¡å¼
        list_patterns = [
            "æœ‰å“ªäº›", "å“ªäº›æ–‡æ¡£", "ç›¸å…³æ–‡æ¡£", "æ–‡æ¡£åˆ—è¡¨", "åˆ—å‡º", 
            "æ‰¾åˆ°", "æœç´¢", "æŸ¥æ‰¾", "æ–‡æ¡£", "å“ªäº›æ–‡æ¡£",
            "ä»€ä¹ˆæ–‡æ¡£", "æœ‰ä»€ä¹ˆæ–‡æ¡£", "åŒ…å«å“ªäº›", "æ¶‰åŠå“ªäº›",
            "what documents", "list", "find documents", "search documents",
            "ç›¸å…³", "å…³äº.*çš„æ–‡æ¡£", ".*æ–‡æ¡£.*æœ‰å“ªäº›"
        ]
        
        # ç»Ÿè®¡æŸ¥è¯¢çš„å…³é”®è¯
        stats_patterns = [
            "æœ‰å¤šå°‘", "æ•°é‡", "ç»Ÿè®¡", "æ€»æ•°", "å‡ ä¸ª", "å¤šå°‘æ–‡æ¡£",
            "how many", "count", "number of"
        ]
        
        # å¯¹æ¯”æŸ¥è¯¢çš„å…³é”®è¯
        comparison_patterns = [
            "å¯¹æ¯”", "åŒºåˆ«", "å·®å¼‚", "æ¯”è¾ƒ", "vs", "versus", "å’Œ.*çš„åŒºåˆ«",
            "compare", "difference", "vs"
        ]
        
        # æ£€æŸ¥æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢
        list_score = 0.0
        for pattern in list_patterns:
            if pattern in question_lower:
                list_score += 0.3
                if pattern in ["æœ‰å“ªäº›", "å“ªäº›æ–‡æ¡£", "æ–‡æ¡£åˆ—è¡¨", "list"]:
                    list_score += 0.4  # æ›´å¼ºçš„ä¿¡å·
        
        # æ£€æŸ¥ç»Ÿè®¡æŸ¥è¯¢
        stats_score = 0.0
        for pattern in stats_patterns:
            if pattern in question_lower:
                stats_score += 0.5
        
        # æ£€æŸ¥å¯¹æ¯”æŸ¥è¯¢
        comparison_score = 0.0
        for pattern in comparison_patterns:
            if pattern in question_lower:
                comparison_score += 0.5
        
        # æå–å…³é”®è¯ï¼ˆç”¨äºåç»­æœç´¢ï¼‰
        keywords = self._extract_keywords(question)
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹
        if list_score >= 0.5:
            return {
                "type": "document_list",
                "confidence": min(list_score, 1.0),
                "keywords": keywords,
                "subtype": "stats" if stats_score > 0.3 else "list"
            }
        elif stats_score >= 0.3:
            return {
                "type": "document_list",  # ç»Ÿè®¡æŸ¥è¯¢ä¹Ÿå½’ç±»ä¸ºæ–‡æ¡£åˆ—è¡¨
                "confidence": min(stats_score, 1.0),
                "keywords": keywords,
                "subtype": "stats"
            }
        elif comparison_score >= 0.3:
            return {
                "type": "content_qa",  # å¯¹æ¯”æŸ¥è¯¢éœ€è¦å†…å®¹åˆ†æ
                "confidence": min(comparison_score, 1.0),
                "keywords": keywords,
                "subtype": "comparison"
            }
        else:
            # é»˜è®¤æ˜¯å†…å®¹é—®ç­”
            return {
                "type": "content_qa",
                "confidence": 0.5,
                "keywords": keywords,
                "subtype": "normal"
            }
    
    def _analyze_question_with_ai(self, question: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†æé—®é¢˜å¹¶æå–æœç´¢å…³é”®è¯å’Œç­–ç•¥ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«å…³é”®è¯ã€æœç´¢æŸ¥è¯¢å’Œç›¸å…³æ¦‚å¿µçš„å­—å…¸
        """
        try:
            from infrastructure.llm.service import LLMService
            import json
            import re
            
            llm_service = LLMService()
            
            prompt = f"""åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œæå–ç”¨äºæœç´¢çŸ¥è¯†åº“çš„å…³é”®è¯å’ŒæŸ¥è¯¢ç­–ç•¥ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åˆ†æï¼š
1. é—®é¢˜çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. éœ€è¦æœç´¢å“ªäº›å…³é”®è¯ï¼Ÿï¼ˆæå–2-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼‰
3. æœ‰å“ªäº›åŒä¹‰è¯æˆ–ç›¸å…³æ¦‚å¿µï¼Ÿ
4. å¯ä»¥å°è¯•å“ªäº›ä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Ÿï¼ˆç”Ÿæˆ3-5ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸé—®é¢˜çš„ä¸åŒè¡¨è¾¾æ–¹å¼ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "search_queries": ["æœç´¢æŸ¥è¯¢1", "æœç´¢æŸ¥è¯¢2", "æœç´¢æŸ¥è¯¢3"],
    "related_concepts": ["ç›¸å…³æ¦‚å¿µ1", "ç›¸å…³æ¦‚å¿µ2"]
}}

è¦æ±‚ï¼š
- keywordsï¼šæå–çš„æ ¸å¿ƒå…³é”®è¯ï¼Œå»é™¤ç–‘é—®è¯ï¼ˆä»€ä¹ˆã€å¦‚ä½•ã€æ€ä¹ˆç­‰ï¼‰
- search_queriesï¼šå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸé—®é¢˜çš„ä¸åŒè¡¨è¾¾æ–¹å¼ã€ç®€åŒ–ç‰ˆæœ¬ã€å…³é”®è¯ç»„åˆç­‰
- related_conceptsï¼šç›¸å…³æ¦‚å¿µæˆ–åŒä¹‰è¯

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
            
            log.info("ä½¿ç”¨AIåˆ†æé—®é¢˜å¹¶æå–æœç´¢ç­–ç•¥...")
            response = llm_service.generate(prompt)
            
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            json_match = re.search(r'\{[^{}]*"keywords"[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•è§£ææ•´ä¸ªå“åº”
                json_str = response.strip()
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                json_str = re.sub(r'```json\s*', '', json_str)
                json_str = re.sub(r'```\s*', '', json_str)
                json_str = json_str.strip()
            
            try:
                result = json.loads(json_str)
                
                # éªŒè¯å’Œæ¸…ç†ç»“æœ
                keywords = result.get("keywords", [])
                search_queries = result.get("search_queries", [])
                related_concepts = result.get("related_concepts", [])
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœç´¢æŸ¥è¯¢
                if not search_queries:
                    search_queries = [question]
                else:
                    # ç¡®ä¿åŸé—®é¢˜åœ¨æœç´¢æŸ¥è¯¢ä¸­
                    if question not in search_queries:
                        search_queries.insert(0, question)
                
                # é™åˆ¶æ•°é‡
                keywords = keywords[:5]
                search_queries = search_queries[:5]
                related_concepts = related_concepts[:3]
                
                return {
                    "keywords": keywords,
                    "search_queries": search_queries,
                    "related_concepts": related_concepts,
                }
            except json.JSONDecodeError as e:
                log.warning(f"AIè¿”å›çš„JSONè§£æå¤±è´¥: {e}ï¼Œå“åº”: {response[:200]}")
                # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
                return self._fallback_extract_keywords(question)
                
        except Exception as e:
            log.warning(f"AIåˆ†æé—®é¢˜å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–")
            # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
            return self._fallback_extract_keywords(question)
    
    def _fallback_extract_keywords(self, question: str) -> Dict[str, Any]:
        """
        å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«å…³é”®è¯å’Œæœç´¢æŸ¥è¯¢çš„å­—å…¸
        """
        keywords = self._extract_keywords(question)
        search_queries = [question] + keywords[:2]
        
        return {
            "keywords": keywords,
            "search_queries": search_queries,
            "related_concepts": [],
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ç©ºæ ¼
        text_clean = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        
        keywords = []
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ç¬¦ï¼Œé¿å…æå–æ•´ä¸ªé—®é¢˜ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', text_clean)
        keywords.extend(chinese_words)
        
        # æå–è‹±æ–‡å•è¯ï¼ˆ3ä¸ªå­—ç¬¦ä»¥ä¸Šï¼‰
        english_words = re.findall(r'\b[a-zA-Z]{3,}\b', text_clean)
        keywords.extend(english_words)
        
        # è¿‡æ»¤å¸¸è§åœç”¨è¯å’Œç–‘é—®è¯
        stop_words = {
            'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›', 'è¿™ä¸ª', 'é‚£ä¸ª', 
            'æ˜¯', 'çš„', 'äº†', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä¸º',
            'æ˜¯ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›',
            'the', 'is', 'are', 'a', 'an', 'and', 'or', 'what', 'how', 'why'
        }
        keywords = [kw for kw in keywords if kw not in stop_words and len(kw) >= 2]
        
        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šå¦‚æœå…³é”®è¯åŒ…å«åœç”¨è¯ï¼Œå°è¯•æå–æ ¸å¿ƒéƒ¨åˆ†
        filtered_keywords = []
        for kw in keywords:
            # ç§»é™¤å¸¸è§çš„ç–‘é—®è¯å‰ç¼€/åç¼€
            kw_clean = kw
            for stop in ['ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ˜¯', 'çš„']:
                if kw_clean.startswith(stop):
                    kw_clean = kw_clean[len(stop):]
                if kw_clean.endswith(stop):
                    kw_clean = kw_clean[:-len(stop)]
            if kw_clean and len(kw_clean) >= 2 and kw_clean not in stop_words:
                filtered_keywords.append(kw_clean)
        
        keywords = filtered_keywords if filtered_keywords else keywords
        
        # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
        seen = set()
        unique_keywords = []
        for kw in keywords:
            kw_lower = kw.lower()
            if kw_lower not in seen:
                seen.add(kw_lower)
                unique_keywords.append(kw)
        
        # å¦‚æœæå–çš„å…³é”®è¯å¤ªå°‘ï¼Œå°è¯•æå–2-3å­—çš„çŸ­è¯­
        if len(unique_keywords) < 2:
            # æå–2-3å­—çš„ä¸­æ–‡çŸ­è¯­
            phrases = re.findall(r'[\u4e00-\u9fff]{2,3}', text)
            for phrase in phrases:
                if phrase not in stop_words and phrase not in seen:
                    seen.add(phrase.lower())
                    unique_keywords.append(phrase)
                    if len(unique_keywords) >= 3:
                        break
        
        return unique_keywords[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®è¯
    
    def _format_document_list(self, documents: List[Dict[str, Any]], question: str, subtype: str = "list") -> str:
        """
        æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ä¸ºç­”æ¡ˆæ–‡æœ¬ã€‚
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            question: ç”¨æˆ·é—®é¢˜
            subtype: é—®é¢˜å­ç±»å‹ï¼ˆlist/statsï¼‰
            
        Returns:
            æ ¼å¼åŒ–åçš„ç­”æ¡ˆæ–‡æœ¬
        """
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        # ç»Ÿè®¡æŸ¥è¯¢
        if subtype == "stats":
            answer = f"æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š\n\n"
        else:
            answer = f"æ‰¾åˆ°ä»¥ä¸‹ {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š\n\n"
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„ï¼ˆé«˜/ä¸­/ä½ï¼‰
        high_relevance = [d for d in documents if d.get("similarity", 0) >= 0.5]
        medium_relevance = [d for d in documents if 0.3 <= d.get("similarity", 0) < 0.5]
        low_relevance = [d for d in documents if d.get("similarity", 0) < 0.3]
        
        # æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨
        doc_index = 1
        if high_relevance:
            answer += "**é«˜ç›¸å…³æ€§æ–‡æ¡£ï¼š**\n"
            for doc in high_relevance:
                similarity = doc.get("similarity", 0)
                similarity_str = f"ï¼ˆç›¸å…³æ€§: {similarity:.1%}ï¼‰" if similarity > 0 else ""
                answer += f"{doc_index}. {doc['title']}{similarity_str}\n"
                doc_index += 1
            answer += "\n"
        
        if medium_relevance:
            answer += "**ä¸­ç­‰ç›¸å…³æ€§æ–‡æ¡£ï¼š**\n"
            for doc in medium_relevance:
                similarity = doc.get("similarity", 0)
                similarity_str = f"ï¼ˆç›¸å…³æ€§: {similarity:.1%}ï¼‰" if similarity > 0 else ""
                answer += f"{doc_index}. {doc['title']}{similarity_str}\n"
                doc_index += 1
            answer += "\n"
        
        if low_relevance:
            answer += "**å…¶ä»–ç›¸å…³æ–‡æ¡£ï¼š**\n"
            for doc in low_relevance:
                similarity = doc.get("similarity", 0)
                similarity_str = f"ï¼ˆç›¸å…³æ€§: {similarity:.1%}ï¼‰" if similarity > 0 else ""
                answer += f"{doc_index}. {doc['title']}{similarity_str}\n"
                doc_index += 1
        
        # æ·»åŠ æç¤º
        answer += "\nğŸ’¡ æç¤ºï¼šç‚¹å‡»æ–‡æ¡£æ ‡é¢˜å¯ä»¥æŸ¥çœ‹å®Œæ•´å†…å®¹ã€‚"
        
        return answer
    
    def _extract_relevant_chunk(self, content: str, question: str, keywords: List[str], chunk_size: int = 4000) -> str:
        """
        ä»æ–‡æ¡£ä¸­æå–ä¸é—®é¢˜æœ€ç›¸å…³çš„ç‰‡æ®µã€‚
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            question: ç”¨æˆ·é—®é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            chunk_size: ç‰‡æ®µå¤§å°ï¼ˆå¢åŠ åˆ°2000ä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
            
        Returns:
            ç›¸å…³ç‰‡æ®µ
        """
        import re
        
        if not content:
            return ""
        
        # å¦‚æœæ–‡æ¡£è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›
        if len(content) <= chunk_size:
            return content
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n+', content)
        
        # è®¡ç®—æ¯ä¸ªæ®µè½çš„ç›¸å…³æ€§åˆ†æ•°
        scored_paragraphs = []
        for para in paragraphs:
            if not para.strip():
                continue
            
            score = 0
            para_lower = para.lower()
            question_lower = question.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„å…³é”®è¯
            for keyword in keywords:
                if keyword.lower() in para_lower:
                    score += 2
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„å®Œæ•´çŸ­è¯­
            if question_lower in para_lower:
                score += 5
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜ä¸­çš„éƒ¨åˆ†è¯æ±‡
            question_words = question_lower.split()
            for word in question_words:
                if len(word) >= 2 and word in para_lower:
                    score += 1
            
            if score > 0:
                scored_paragraphs.append((score, para))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½ç»„åˆï¼ˆå¢åŠ åˆ°æœ€å¤š15ä¸ªæ®µè½ï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
        # ä½¿ç”¨embeddingè®¡ç®—æ®µè½ç›¸ä¼¼åº¦ï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…
        selected_text = ""
        selected_count = 0
        max_paragraphs = 15  # å¢åŠ æ®µè½æ•°é‡
        
        # å¦‚æœæ®µè½æ•°é‡è¾ƒå¤šï¼Œå°è¯•ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ›´å‡†ç¡®ï¼‰
        if len(scored_paragraphs) > 5:
            try:
                from infrastructure.embedding.service import EmbeddingService
                import numpy as np
                
                embedding_service = EmbeddingService()
                question_vector = np.array(embedding_service.embed_text(question))
                
                # é‡æ–°è®¡ç®—æ¯ä¸ªæ®µè½çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç»“åˆå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
                enhanced_scored_paragraphs = []
                for score, para in scored_paragraphs[:20]:  # åªå¤„ç†å‰20ä¸ªæ®µè½ä»¥æé«˜æ€§èƒ½
                    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                    para_vector = np.array(embedding_service.embed_text(para[:500]))
                    semantic_score = np.dot(question_vector, para_vector) / (
                        np.linalg.norm(question_vector) * np.linalg.norm(para_vector) + 1e-8
                    )
                    # ç»“åˆå…³é”®è¯åŒ¹é…åˆ†æ•°å’Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
                    combined_score = score * 0.4 + semantic_score * 100 * 0.6
                    enhanced_scored_paragraphs.append((combined_score, para))
                
                # é‡æ–°æ’åº
                enhanced_scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
                scored_paragraphs = enhanced_scored_paragraphs
            except Exception as e:
                log.debug(f"ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…: {e}")
                # å¦‚æœå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¥çš„å…³é”®è¯åŒ¹é…ç»“æœ
        
        # é€‰æ‹©æ®µè½æ—¶ï¼Œä¿ç•™ä¸Šä¸‹æ–‡çª—å£ï¼ˆæ¯ä¸ªç›¸å…³æ®µè½å‰åå„ä¿ç•™ä¸€ä¸ªæ®µè½ï¼‰
        selected_indices = set()
        
        # æ„å»ºæ®µè½åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
        para_to_index = {}
        for idx, orig_para in enumerate(paragraphs):
            if orig_para.strip():
                para_key = orig_para.strip()[:100]  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºkey
                if para_key not in para_to_index:
                    para_to_index[para_key] = []
                para_to_index[para_key].append(idx)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½åŠå…¶ä¸Šä¸‹æ–‡
        for score, para in scored_paragraphs[:max_paragraphs]:
            # æ‰¾åˆ°è¿™ä¸ªæ®µè½åœ¨åŸæ–‡ä¸­çš„ç´¢å¼•
            para_key = para.strip()[:100]
            para_indices = para_to_index.get(para_key, [])
            
            if para_indices:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç´¢å¼•
                para_index = para_indices[0]
                # é€‰æ‹©å½“å‰æ®µè½åŠå…¶å‰åå„ä¸€ä¸ªæ®µè½ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
                for ctx_idx in range(max(0, para_index - 1), min(len(paragraphs), para_index + 2)):
                    selected_indices.add(ctx_idx)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                para_stripped = para.strip()
                for idx, orig_para in enumerate(paragraphs):
                    if orig_para.strip() and para_stripped[:50] in orig_para.strip():
                        para_index = idx
                        # é€‰æ‹©å½“å‰æ®µè½åŠå…¶å‰åå„ä¸€ä¸ªæ®µè½
                        for ctx_idx in range(max(0, para_index - 1), min(len(paragraphs), para_index + 2)):
                            selected_indices.add(ctx_idx)
                        break
        
        # æŒ‰é¡ºåºæå–é€‰ä¸­çš„æ®µè½
        for idx in sorted(selected_indices):
            para = paragraphs[idx]
            if not para.strip():
                continue
            
            if len(selected_text) + len(para) <= chunk_size:
                selected_text += para + "\n\n"
            else:
                # å¦‚æœè¶…è¿‡é•¿åº¦é™åˆ¶ï¼Œå°è¯•æˆªå–éƒ¨åˆ†
                remaining = chunk_size - len(selected_text)
                if remaining > 200:  # è‡³å°‘ä¿ç•™200å­—ç¬¦
                    selected_text += para[:remaining] + "..."
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ®µè½ï¼Œè¿”å›å¼€å¤´éƒ¨åˆ†ï¼ˆå¢åŠ é•¿åº¦ï¼‰
        if not selected_text:
            # è¿”å›æ›´å¤šå†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æ¡£å¼€å¤´
            selected_text = content[:chunk_size] + "..."
        
        return selected_text.strip()
    
    def _calculate_similarity(self, question: str, content: str) -> float:
        """
        è®¡ç®—é—®é¢˜å’Œæ–‡æ¡£å†…å®¹çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨embeddingï¼‰ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            content: æ–‡æ¡£å†…å®¹
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        try:
            from infrastructure.embedding.service import EmbeddingService
            import numpy as np
            
            # åˆå§‹åŒ–embeddingæœåŠ¡
            embedding_service = EmbeddingService()
            
            # å‘é‡åŒ–é—®é¢˜å’Œå†…å®¹
            # æ³¨æ„ï¼šcontentåº”è¯¥æ˜¯å·²ç»æå–çš„ç›¸å…³ç‰‡æ®µï¼Œä¸éœ€è¦å†æˆªå–å‰500å­—ç¬¦
            # å¦‚æœcontentå¤ªé•¿ï¼ˆè¶…è¿‡2000å­—ç¬¦ï¼‰ï¼Œæˆªå–å‰2000å­—ç¬¦ä»¥æé«˜æ€§èƒ½
            content_to_embed = content[:2000] if len(content) > 2000 else content
            
            # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            if not content_to_embed or not content_to_embed.strip():
                log.warning(f"å†…å®¹ä¸ºç©ºï¼Œè¿”å›ç›¸ä¼¼åº¦0.0")
                return 0.0
            
            # è®°å½•embeddingæœåŠ¡ä¿¡æ¯
            model_name = embedding_service.get_model_name()
            log.debug(f"ä½¿ç”¨embeddingæ¨¡å‹: {model_name}")
            
            # å‘é‡åŒ–é—®é¢˜
            question_vector_raw = embedding_service.embed_text(question)
            question_vector = np.array(question_vector_raw)
            
            # å‘é‡åŒ–å†…å®¹
            content_vector_raw = embedding_service.embed_text(content_to_embed)
            content_vector = np.array(content_vector_raw)
            
            # éªŒè¯å‘é‡æ˜¯å¦æœ‰æ•ˆ
            if question_vector.size == 0 or content_vector.size == 0:
                log.warning(f"å‘é‡ä¸ºç©ºï¼Œè¿”å›ç›¸ä¼¼åº¦0.0 (question_size={question_vector.size}, content_size={content_vector.size})")
                return 0.0
            
            # æ£€æŸ¥å‘é‡ç»´åº¦æ˜¯å¦åŒ¹é…
            if question_vector.shape != content_vector.shape:
                log.error(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: question={question_vector.shape}, content={content_vector.shape}")
                return 0.0
            
            # æ£€æŸ¥å‘é‡æ˜¯å¦å…¨ä¸ºé›¶
            if np.all(question_vector == 0) or np.all(content_vector == 0):
                log.warning(f"æ£€æµ‹åˆ°é›¶å‘é‡: question_all_zero={np.all(question_vector == 0)}, content_all_zero={np.all(content_vector == 0)}")
                log.warning(f"é—®é¢˜å‘é‡å‰5ä¸ªå€¼: {question_vector[:5]}")
                log.warning(f"å†…å®¹å‘é‡å‰5ä¸ªå€¼: {content_vector[:5]}")
                # å¦‚æœå‘é‡å…¨ä¸ºé›¶ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºå›é€€
                keywords = self._extract_keywords(question)
                content_lower = content.lower() if content else ""
                match_count = sum(1 for kw in keywords if kw.lower() in content_lower)
                estimated_similarity = min(0.4, 0.1 + match_count * 0.05)
                log.info(f"æ£€æµ‹åˆ°é›¶å‘é‡ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä¼°è®¡ç›¸ä¼¼åº¦: {estimated_similarity:.3f} (åŒ¹é…å…³é”®è¯æ•°: {match_count})")
                return estimated_similarity
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(question_vector, content_vector)
            norm_q = np.linalg.norm(question_vector)
            norm_c = np.linalg.norm(content_vector)
            
            if norm_q == 0 or norm_c == 0:
                log.warning(f"å‘é‡æ¨¡é•¿ä¸º0ï¼Œè¿”å›ç›¸ä¼¼åº¦0.0 (norm_q={norm_q}, norm_c={norm_c})")
                return 0.0
            
            similarity = dot_product / (norm_q * norm_c)
            
            # ğŸ”´ ä¿®å¤ï¼šå¤„ç†è´Ÿæ•°ç›¸ä¼¼åº¦
            # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´æ˜¯-1åˆ°1ï¼Œè´Ÿæ•°è¡¨ç¤ºå‘é‡æ–¹å‘ç›¸åæˆ–æ¥è¿‘å‚ç›´
            # è´Ÿæ•°ç›¸ä¼¼åº¦åº”è¯¥è¢«è§†ä¸ºä½ç›¸å…³æ€§ï¼Œä½†ä¸åº”è¯¥è¢«ç›´æ¥æˆªæ–­ä¸º0.0
            if similarity < 0:
                # è´Ÿæ•°ç›¸ä¼¼åº¦è¡¨ç¤ºä¸ç›¸å…³ï¼Œè®¾ä¸º0.0
                # ä½†è®°å½•æ—¥å¿—ä»¥ä¾¿æ’æŸ¥é—®é¢˜
                log.debug(f"æ£€æµ‹åˆ°è´Ÿæ•°ç›¸ä¼¼åº¦: {similarity:.3f} (é—®é¢˜: {question[:50]}..., å†…å®¹é•¿åº¦: {len(content_to_embed)})")
                log.debug(f"ç‚¹ç§¯: {dot_product:.3f}, norm_q: {norm_q:.3f}, norm_c: {norm_c:.3f}")
                similarity = 0.0
            else:
            # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨0-1èŒƒå›´å†…
                similarity = min(1.0, float(similarity))
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼ˆä»…åœ¨ç›¸ä¼¼åº¦å¼‚å¸¸æ—¶ï¼‰
            if similarity < 0.1:
                log.debug(f"ç›¸ä¼¼åº¦è¾ƒä½: {similarity:.3f} (é—®é¢˜: {question[:50]}..., å†…å®¹é•¿åº¦: {len(content_to_embed)}, å‘é‡ç»´åº¦: {question_vector.shape[0]})")
                log.debug(f"é—®é¢˜å‘é‡ç»Ÿè®¡: min={question_vector.min():.3f}, max={question_vector.max():.3f}, mean={question_vector.mean():.3f}")
                log.debug(f"å†…å®¹å‘é‡ç»Ÿè®¡: min={content_vector.min():.3f}, max={content_vector.max():.3f}, mean={content_vector.mean():.3f}")
            
            return similarity
            
        except Exception as e:
            log.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä¼°è®¡å€¼")
            import traceback
            log.debug(traceback.format_exc())
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼ŒåŸºäºå…³é”®è¯åŒ¹é…è¿”å›ä¸€ä¸ªä¼°è®¡å€¼ï¼ˆä½†æ ‡è®°ä¸ºä½ç›¸ä¼¼åº¦ï¼‰
            keywords = self._extract_keywords(question)
            content_lower = content.lower() if content else ""
            match_count = sum(1 for kw in keywords if kw.lower() in content_lower)
            # é™ä½é»˜è®¤å€¼ï¼Œé¿å…è¯¯åˆ¤ä¸ºç›¸å…³
            estimated_similarity = min(0.4, 0.1 + match_count * 0.05)  # åŸºç¡€åˆ†æ•°0.1ï¼Œæ¯ä¸ªå…³é”®è¯åŒ¹é…+0.05ï¼Œæœ€é«˜0.4
            log.info(f"ä½¿ç”¨å…³é”®è¯åŒ¹é…ä¼°è®¡ç›¸ä¼¼åº¦: {estimated_similarity:.3f} (åŒ¹é…å…³é”®è¯æ•°: {match_count})")
            return estimated_similarity
    
    def _analyze_search_results_with_ai(
        self, 
        question: str, 
        search_results: List[Dict[str, Any]], 
        keywords: List[str],
        related_concepts: List[str]
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨AIåˆ†ææœç´¢ç»“æœçš„ç›¸å…³æ€§å’Œå…³é”®ä¿¡æ¯ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            keywords: æå–çš„å…³é”®è¯
            related_concepts: ç›¸å…³æ¦‚å¿µ
            
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
            - relevance_summary: ç›¸å…³æ€§æ€»ç»“
            - key_points: å…³é”®ä¿¡æ¯ç‚¹
            - answer_strategy: ç­”æ¡ˆç”Ÿæˆç­–ç•¥
        """
        try:
            from infrastructure.llm.service import LLMService
            import json
            import re
            
            llm_service = LLMService()
            
            # æ„å»ºæœç´¢ç»“æœæ‘˜è¦ï¼ˆåªåŒ…å«æ ‡é¢˜å’Œç›¸ä¼¼åº¦ï¼Œé¿å…tokenè¿‡å¤šï¼‰
            results_summary = []
            for i, result in enumerate(search_results[:5], 1):
                results_summary.append({
                    "åºå·": i,
                    "æ ‡é¢˜": result.get("title", "æœªçŸ¥"),
                    "ç›¸ä¼¼åº¦": f"{result.get('similarity', 0):.2f}",
                    "å†…å®¹æ‘˜è¦": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œéœ€è¦åˆ†ææœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€ç›¸å…³æ¦‚å¿µã€‘
{', '.join(related_concepts) if related_concepts else 'æ— '}

ã€æœç´¢ç»“æœã€‘
{json.dumps(results_summary, ensure_ascii=False, indent=2)}

è¯·åˆ†æï¼š
1. è¿™äº›æœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§å¦‚ä½•ï¼Ÿ
2. å“ªäº›ç»“æœæœ€ç›¸å…³ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
3. ä»è¿™äº›ç»“æœä¸­å¯ä»¥æå–å“ªäº›å…³é”®ä¿¡æ¯ç‚¹ï¼Ÿ
4. åº”è¯¥å¦‚ä½•ç»„ç»‡ç­”æ¡ˆï¼Ÿï¼ˆç›´æ¥å›ç­”ã€åˆ†ç‚¹è¯´æ˜ã€å¯¹æ¯”è¯´æ˜ç­‰ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "relevance_summary": "ç›¸å…³æ€§æ€»ç»“ï¼ˆ1-2å¥è¯ï¼‰",
    "key_points": ["å…³é”®ä¿¡æ¯ç‚¹1", "å…³é”®ä¿¡æ¯ç‚¹2", "å…³é”®ä¿¡æ¯ç‚¹3"],
    "answer_strategy": "ç­”æ¡ˆç”Ÿæˆç­–ç•¥ï¼ˆå¦‚ï¼šç›´æ¥å›ç­”ã€åˆ†ç‚¹è¯´æ˜ã€å¯¹æ¯”è¯´æ˜ç­‰ï¼‰",
    "most_relevant_results": [1, 2]  // æœ€ç›¸å…³çš„ç»“æœåºå·åˆ—è¡¨
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
            
            log.info("ä½¿ç”¨AIåˆ†ææœç´¢ç»“æœ...")
            response = llm_service.generate(prompt)
            
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            json_match = re.search(r'\{[^{}]*"relevance_summary"[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                json_str = response.strip()
                json_str = re.sub(r'```json\s*', '', json_str)
                json_str = re.sub(r'```\s*', '', json_str)
                json_str = json_str.strip()
            
            try:
                result = json.loads(json_str)
                return result
            except json.JSONDecodeError as e:
                log.warning(f"AIåˆ†æç»“æœJSONè§£æå¤±è´¥: {e}ï¼Œå“åº”: {response[:200]}")
                return {
                    "relevance_summary": "æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³",
                    "key_points": [],
                    "answer_strategy": "ç›´æ¥å›ç­”",
                    "most_relevant_results": [1, 2, 3],
                }
                
        except Exception as e:
            log.warning(f"AIåˆ†ææœç´¢ç»“æœå¤±è´¥: {e}")
            return {
                "relevance_summary": "æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³",
                "key_points": [],
                "answer_strategy": "ç›´æ¥å›ç­”",
                "most_relevant_results": [1, 2, 3],
            }
    
    def _build_answer_prompt(
        self,
        question: str,
        context: str,
        search_results: List[Dict[str, Any]],
        analysis_result: Dict[str, Any],
        keywords: List[str]
    ) -> str:
        """
        æ„å»ºç­”æ¡ˆç”Ÿæˆçš„Promptï¼Œè®©AIæ›´å¥½åœ°åˆ©ç”¨æœç´¢ç»“æœã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            analysis_result: AIåˆ†æç»“æœ
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            å®Œæ•´çš„Prompt
        """
        # æ„å»ºæœç´¢ç»“æœä¿¡æ¯
        results_info = []
        for i, result in enumerate(search_results, 1):
            similarity = result.get("similarity", 0)
            title = result.get("title", "æœªçŸ¥")
            results_info.append(f"{i}. {title} (ç›¸ä¼¼åº¦: {similarity:.2f})")
        
        results_summary = "\n".join(results_info)
        
        # è·å–AIåˆ†æçš„å…³é”®ä¿¡æ¯
        relevance_summary = analysis_result.get("relevance_summary", "")
        key_points = analysis_result.get("key_points", [])
        answer_strategy = analysis_result.get("answer_strategy", "ç›´æ¥å›ç­”")
        most_relevant = analysis_result.get("most_relevant_results", [])
        
        # æ„å»ºå…³é”®ä¿¡æ¯ç‚¹
        key_points_text = ""
        if key_points:
            key_points_text = "\n".join([f"- {point}" for point in key_points[:5]])
        
        # æ„å»ºæœ€ç›¸å…³ç»“æœæç¤º
        most_relevant_text = ""
        if most_relevant:
            most_relevant_titles = [
                search_results[i-1].get("title", "") 
                for i in most_relevant 
                if 1 <= i <= len(search_results)
            ]
            if most_relevant_titles:
                most_relevant_text = f"\nã€æœ€ç›¸å…³æ–‡æ¡£ã€‘ä¼˜å…ˆå‚è€ƒä»¥ä¸‹æ–‡æ¡£ï¼š{', '.join(most_relevant_titles)}"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AIçŸ¥è¯†åº“åŠ©æ‰‹ï¼Œæ“…é•¿æ·±å…¥åˆ†ææ–‡æ¡£å†…å®¹å¹¶ç”Ÿæˆé«˜è´¨é‡ã€ç»“æ„åŒ–çš„ç­”æ¡ˆã€‚

ã€ä»»åŠ¡ã€‘
åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ·±å…¥åˆ†æå¹¶å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä½ éœ€è¦ï¼š
1. **æ·±å…¥ç†è§£**ï¼šä»”ç»†é˜…è¯»æ–‡æ¡£å†…å®¹ï¼Œç†è§£ä¸Šä¸‹æ–‡å’Œç»†èŠ‚
2. **æå–å…³é”®ä¿¡æ¯**ï¼šè¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯ã€å…³é”®æ­¥éª¤ã€é‡è¦æ¦‚å¿µ
3. **ç»¼åˆåˆ†æ**ï¼šå¦‚æœæ¶‰åŠå¤šä¸ªæ–‡æ¡£ï¼Œè¦ç»¼åˆä¸åŒæ–‡æ¡£çš„ä¿¡æ¯ï¼Œå½¢æˆå®Œæ•´çš„ç­”æ¡ˆ
4. **ç»“æ„åŒ–ç»„ç»‡**ï¼šæŒ‰ç…§é€»è¾‘é¡ºåºç»„ç»‡ç­”æ¡ˆï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½å’Œåˆ†ç‚¹è¯´æ˜
5. **æ·±å…¥é˜è¿°**ï¼šä¸ä»…è¦å¼•ç”¨æ–‡æ¡£å†…å®¹ï¼Œè¿˜è¦è¿›è¡Œè§£é‡Šã€åˆ†æå’Œæ€»ç»“

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æå–çš„å…³é”®è¯ã€‘
{', '.join(keywords) if keywords else 'æ— '}

ã€æœç´¢ç»“æœåˆ†æã€‘
{relevance_summary if relevance_summary else 'æœç´¢ç»“æœä¸é—®é¢˜ç›¸å…³'}

ã€å…³é”®ä¿¡æ¯ç‚¹ã€‘
{key_points_text if key_points_text else 'éœ€è¦ä»æ–‡æ¡£ä¸­æå–'}

ã€ç­”æ¡ˆç”Ÿæˆç­–ç•¥ã€‘
{answer_strategy}{most_relevant_text}

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€æœç´¢ç»“æœåˆ—è¡¨ã€‘
{results_summary}

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **æ·±åº¦åˆ†æ**ï¼š
   - ä¸è¦ç®€å•å¼•ç”¨æ–‡æ¡£ä¸­çš„ä¸€ä¸¤å¥è¯
   - è¦æ·±å…¥ç†è§£æ–‡æ¡£å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œè§£é‡Š
   - å¦‚æœæ–‡æ¡£æåˆ°æŸä¸ªåŠŸèƒ½æˆ–æ¦‚å¿µï¼Œè¦è¯¦ç»†è¯´æ˜å…¶ä½œç”¨ã€ä½¿ç”¨æ–¹æ³•ã€æ³¨æ„äº‹é¡¹ç­‰

2. **å®Œæ•´æ€§**ï¼š
   - å¦‚æœæ–‡æ¡£ä¸­æœ‰å¤šä¸ªç›¸å…³ä¿¡æ¯ç‚¹ï¼Œè¦å…¨éƒ¨æå–å¹¶ç»¼åˆå›ç­”
   - ä¸è¦é—æ¼é‡è¦çš„ç»†èŠ‚ã€æ­¥éª¤ã€æ¡ä»¶ã€é™åˆ¶ç­‰
   - å¦‚æœæ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œè¦å…¨é¢è¦†ç›–

3. **ç»“æ„åŒ–ç»„ç»‡**ï¼š
   - ä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„
   - å¯¹äºå¤æ‚é—®é¢˜ï¼Œä½¿ç”¨åˆ†ç‚¹è¯´æ˜ï¼ˆ1. 2. 3.ï¼‰æˆ–åˆ†ç±»è¯´æ˜
   - æŒ‰ç…§é€»è¾‘é¡ºåºç»„ç»‡ï¼šæ¦‚è¿° â†’ è¯¦ç»†è¯´æ˜ â†’ æ€»ç»“

4. **å¯è¯»æ€§å’Œä¸“ä¸šæ€§**ï¼š
   - ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œè¯­è¨€æµç•…è‡ªç„¶
   - ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½†ç¡®ä¿æ˜“äºç†è§£
   - é¿å…å†—ä½™å’Œé‡å¤
   - é€‚å½“ä½¿ç”¨è¿‡æ¸¡è¯ï¼Œä½¿ç­”æ¡ˆè¿è´¯

5. **å¼•ç”¨å’Œæ ‡æ³¨**ï¼š
   - åœ¨ç­”æ¡ˆå¼€å¤´æˆ–å…³é”®éƒ¨åˆ†æåŠæ–‡æ¡£æ¥æºï¼ˆå¦‚"æ ¹æ®ã€ŠXXXæ–‡æ¡£ã€‹..."ï¼‰
   - å¦‚æœä¿¡æ¯æ¥è‡ªå¤šä¸ªæ–‡æ¡£ï¼Œå¯ä»¥åˆ†åˆ«æ ‡æ³¨

ã€ç­”æ¡ˆç»“æ„å»ºè®®ã€‘
- **å¼€å¤´**ï¼šç®€è¦è¯´æ˜æ‰¾åˆ°äº†å“ªäº›ç›¸å…³ä¿¡æ¯ï¼ˆå¯æåŠæ–‡æ¡£åç§°ï¼‰
- **ä¸»ä½“**ï¼šè¯¦ç»†å›ç­”é—®é¢˜çš„å„ä¸ªæ–¹é¢ï¼Œä½¿ç”¨åˆ†ç‚¹æˆ–åˆ†æ®µè¯´æ˜
- **ç»“å°¾**ï¼šå¦‚æœ‰å¿…è¦ï¼Œè¿›è¡Œæ€»ç»“æˆ–è¡¥å……è¯´æ˜

ã€æ³¨æ„äº‹é¡¹ã€‘
- **ç›¸å…³æ€§æ£€æŸ¥ï¼ˆæœ€é‡è¦ï¼‰**ï¼š
  - é¦–å…ˆåˆ¤æ–­æ–‡æ¡£å†…å®¹æ˜¯å¦çœŸçš„ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³
  - å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜**å®Œå…¨ä¸ç›¸å…³**æˆ–**ç›¸å…³æ€§å¾ˆä½**ï¼ˆç›¸ä¼¼åº¦<0.5ï¼‰ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯"
  - **ä¸è¦**å¼ºè¡Œå…³è”ä¸ç›¸å…³çš„å†…å®¹
  - **ä¸è¦**åŸºäºä¸ç›¸å…³çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
  - å¦‚æœæ–‡æ¡£ç›¸ä¼¼åº¦å¾ˆä½ï¼Œåº”è¯¥æ˜ç¡®æ‹’ç»å›ç­”ï¼Œè€Œä¸æ˜¯å¼ºè¡Œç”Ÿæˆç­”æ¡ˆ

- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›´æ¥å›ç­”é—®é¢˜çš„ä¿¡æ¯ï¼Œå¯ä»¥åŸºäºç›¸å…³å†…å®¹è¿›è¡Œåˆç†æ¨æ–­ï¼Œä½†è¦è¯´æ˜è¿™æ˜¯åŸºäºæ–‡æ¡£çš„æ¨æ–­
- å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜ä¸å®Œå…¨åŒ¹é…ï¼Œè¯´æ˜æ–‡æ¡£ä¸­æ‰¾åˆ°äº†å“ªäº›ç›¸å…³ä¿¡æ¯ï¼Œå¹¶è§£é‡Šè¿™äº›ä¿¡æ¯å¦‚ä½•å¸®åŠ©å›ç­”é—®é¢˜
- å¦‚æœå¤šä¸ªæ–‡æ¡£æœ‰å†²çªä¿¡æ¯ï¼Œè¦å¯¹æ¯”è¯´æ˜å¹¶æŒ‡å‡ºå·®å¼‚
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

ã€ç­”æ¡ˆã€‘
è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹ï¼Œæ·±å…¥åˆ†æå¹¶å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¦æ±‚ç­”æ¡ˆå®Œæ•´ã€æ·±å…¥ã€æœ‰æ¡ç†ï¼š
"""
        
        return prompt

    def _verify_answer_relevance(self, question: str, answer: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        éªŒè¯ç­”æ¡ˆæ˜¯å¦çœŸçš„å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            éªŒè¯ç»“æœï¼ŒåŒ…å«is_relevantå’Œreason
        """
        try:
            # å¦‚æœæœç´¢ç»“æœçš„å¹³å‡ç›¸ä¼¼åº¦å¾ˆä½ï¼Œç›´æ¥è®¤ä¸ºä¸ç›¸å…³
            if search_results:
                avg_similarity = sum([r.get("similarity", 0) for r in search_results]) / len(search_results)
                if avg_similarity < 0.4:
                    return {
                        "is_relevant": False,
                        "reason": f"æœç´¢ç»“æœå¹³å‡ç›¸ä¼¼åº¦è¿‡ä½ ({avg_similarity:.3f} < 0.4)"
                    }
            
            # æå–é—®é¢˜å…³é”®è¯
            question_keywords = self._extract_keywords(question)
            if not question_keywords:
                return {"is_relevant": True, "reason": "æ— æ³•æå–é—®é¢˜å…³é”®è¯"}
            
            # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«é—®é¢˜çš„ä¸»è¦å…³é”®è¯
            answer_lower = answer.lower()
            matched_keywords = [kw for kw in question_keywords if kw.lower() in answer_lower]
            match_ratio = len(matched_keywords) / len(question_keywords) if question_keywords else 0
            
            # å¦‚æœåŒ¹é…çš„å…³é”®è¯å°‘äº50%ï¼Œè®¤ä¸ºä¸ç›¸å…³
            if match_ratio < 0.5:
                return {
                    "is_relevant": False,
                    "reason": f"ç­”æ¡ˆä¸­åŒ¹é…çš„å…³é”®è¯æ¯”ä¾‹è¿‡ä½ ({match_ratio:.2%} < 50%)"
                }
            
            return {"is_relevant": True, "reason": "ç­”æ¡ˆç›¸å…³æ€§éªŒè¯é€šè¿‡"}
            
        except Exception as e:
            log.warning(f"ç­”æ¡ˆç›¸å…³æ€§éªŒè¯å¤±è´¥: {e}")
            # éªŒè¯å¤±è´¥æ—¶ï¼Œé»˜è®¤è®¤ä¸ºç›¸å…³ï¼ˆé¿å…è¯¯åˆ¤ï¼‰
            return {"is_relevant": True, "reason": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}"}
    
    def _should_use_web_search(self, question: str, kb_result: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨ç½‘ç»œæœç´¢ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            kb_result: çŸ¥è¯†åº“æœç´¢ç»“æœ
            
        Returns:
            æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
        """
        # å¦‚æœçŸ¥è¯†åº“æœç´¢æˆåŠŸä¸”æœ‰ç›¸å…³æ–‡æ¡£ï¼Œæ£€æŸ¥ç›¸ä¼¼åº¦
        if kb_result.get("success") and len(kb_result.get("sources", [])) > 0:
            sources = kb_result.get("sources", [])
            max_similarity = kb_result.get("max_similarity", 0.0)
            
            # å¦‚æœæ²¡æœ‰max_similarityï¼Œä»sourcesè®¡ç®—
            if max_similarity == 0.0:
            max_similarity = max([s.get("similarity", 0) for s in sources])
            
            # å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦>=0.7ï¼Œè®¤ä¸ºçŸ¥è¯†åº“ç»“æœè¶³å¤Ÿå¥½ï¼Œä¸éœ€è¦ç½‘ç»œæœç´¢
            if max_similarity >= 0.7:
                return False
            
            # å¦‚æœç›¸ä¼¼åº¦åœ¨0.6-0.7ä¹‹é—´ï¼Œæ£€æŸ¥ç­”æ¡ˆè´¨é‡
            if max_similarity >= 0.6:
                # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«å¦å®šæ€§è¡¨è¿°
                answer = kb_result.get("answer", "")
                negative_keywords = ["æ²¡æœ‰æ‰¾åˆ°", "æœªæ‰¾åˆ°", "ä¸ç›¸å…³", "æ— æ³•æ‰¾åˆ°", "æ²¡æœ‰ç›¸å…³ä¿¡æ¯"]
                has_negative = any(keyword in answer for keyword in negative_keywords)
                
                # å¦‚æœç­”æ¡ˆåŒ…å«å¦å®šæ€§è¡¨è¿°ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
                if has_negative:
                    log.info(f"ç­”æ¡ˆåŒ…å«å¦å®šæ€§è¡¨è¿°ï¼Œä¸”æ–‡æ¡£ç›¸ä¼¼åº¦ä¸­ç­‰({max_similarity:.3f})ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
                    return True
                
                # åˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜ï¼ˆå¦‚"æ˜¯ä»€ä¹ˆ"ã€"å®šä¹‰"ç­‰ï¼‰
                if self._is_general_concept_question(question):
                    log.info(f"æ£€æµ‹åˆ°é€šç”¨æ¦‚å¿µé—®é¢˜ï¼Œä¸”æ–‡æ¡£ç›¸ä¼¼åº¦ä¸­ç­‰({max_similarity:.3f})ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
                    return True
                
                return False
            
            # å¦‚æœç›¸ä¼¼åº¦åœ¨0.5-0.6ä¹‹é—´ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜
            if max_similarity >= 0.5:
                # åˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜ï¼ˆå¦‚"æ˜¯ä»€ä¹ˆ"ã€"å®šä¹‰"ç­‰ï¼‰
                if self._is_general_concept_question(question):
                    log.info(f"æ£€æµ‹åˆ°é€šç”¨æ¦‚å¿µé—®é¢˜ï¼Œä¸”æ–‡æ¡£ç›¸ä¼¼åº¦è¾ƒä½({max_similarity:.3f})ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
                    return True
            
            # å¦‚æœç›¸ä¼¼åº¦<0.5ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
            if max_similarity < 0.5:
                log.info(f"æ–‡æ¡£ç›¸ä¼¼åº¦è¿‡ä½({max_similarity:.3f})ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
                return True
        
        # å¦‚æœçŸ¥è¯†åº“æœç´¢å¤±è´¥æˆ–æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
        if not kb_result.get("success") or len(kb_result.get("sources", [])) == 0:
            log.info("çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢")
            return True
        
        return False
    
    def _is_general_concept_question(self, question: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æ˜¯å¦æ˜¯é€šç”¨æ¦‚å¿µé—®é¢˜
        """
        # é€šç”¨æ¦‚å¿µé—®é¢˜çš„å…³é”®è¯
        concept_keywords = [
            "æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆæ˜¯", "å®šä¹‰", "å«ä¹‰", "æ„æ€", "æ¦‚å¿µ",
            "ä»‹ç»", "è¯´æ˜", "è§£é‡Š", "å¦‚ä½•ç†è§£", "æ€ä¹ˆç†è§£"
        ]
        
        question_lower = question.lower()
        for keyword in concept_keywords:
            if keyword in question_lower:
                return True
        
        return False
    
    def _search_web_and_merge(self, question: str, kb_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        æœç´¢ç½‘ç»œå¹¶åˆå¹¶ç»“æœã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            kb_result: çŸ¥è¯†åº“æœç´¢ç»“æœ
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        try:
            web_service = self.web_search_service
            if not web_service:
                log.warning("ç½‘ç»œæœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œè¿”å›çŸ¥è¯†åº“ç»“æœ")
                return kb_result
            
            # æœç´¢ç½‘ç»œ
            web_results = web_service.search(question, max_results=5)
            
            if not web_results:
                log.warning("ç½‘ç»œæœç´¢æœªæ‰¾åˆ°ç»“æœï¼Œè¿”å›çŸ¥è¯†åº“ç»“æœ")
                return kb_result
            
            # ä½¿ç”¨LLMåˆå¹¶çŸ¥è¯†åº“å’Œç½‘ç»œæœç´¢ç»“æœ
            from infrastructure.llm.service import LLMService
            llm_service = LLMService()
            
            # æ„å»ºåˆå¹¶æç¤ºè¯
            kb_answer = kb_result.get("answer", "")
            kb_sources = kb_result.get("sources", [])
            
            # æ„å»ºç½‘ç»œæœç´¢ç»“æœæ‘˜è¦
            web_summary = "\n".join([
                f"- {r.get('title', '')}: {r.get('snippet', '')[:200]}..."
                for r in web_results[:3]
            ])
            
            # æ„å»ºåˆå¹¶æç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œéœ€è¦ç»“åˆçŸ¥è¯†åº“ä¿¡æ¯å’Œç½‘ç»œæœç´¢ç»“æœæ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{'æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³æ–‡æ¡£ï¼š' if kb_sources else 'æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£'}
{chr(10).join([f'- {s.get("title", "")} (ç›¸ä¼¼åº¦: {s.get("similarity", 0):.2f})' for s in kb_sources[:3]]) if kb_sources else 'æ— '}

{'ã€çŸ¥è¯†åº“ç­”æ¡ˆã€‘' if kb_answer and kb_result.get('success') else ''}
{kb_answer if kb_answer and kb_result.get('success') else 'çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯'}

ã€ç½‘ç»œæœç´¢ç»“æœã€‘
{web_summary}

ã€è¦æ±‚ã€‘
1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯ï¼ˆå¦‚æœçŸ¥è¯†åº“æœ‰ç›¸å…³ä¿¡æ¯ï¼‰
2. ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœè¡¥å……çŸ¥è¯†åº“ä¿¡æ¯çš„ä¸è¶³
3. æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºï¼š
   - å¦‚æœä¿¡æ¯æ¥è‡ªçŸ¥è¯†åº“ï¼Œæ ‡æ³¨"æ ¹æ®çŸ¥è¯†åº“æ–‡æ¡£..."
   - å¦‚æœä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢ï¼Œæ ‡æ³¨"æ ¹æ®ç½‘ç»œæœç´¢..."
4. å¦‚æœçŸ¥è¯†åº“å’Œç½‘ç»œä¿¡æ¯æœ‰å†²çªï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯
5. ç­”æ¡ˆè¦å®Œæ•´ã€å‡†ç¡®ã€æœ‰æ¡ç†
6. ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”

ã€ç­”æ¡ˆã€‘
è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š
"""
            
            # ç”Ÿæˆåˆå¹¶åçš„ç­”æ¡ˆ
            merged_answer = llm_service.generate(prompt)
            
            # åˆå¹¶æ¥æº
            merged_sources = list(kb_sources)
            for web_result in web_results[:3]:
                merged_sources.append({
                    "title": web_result.get("title", ""),
                    "url": web_result.get("url", ""),
                    "source": "web_search",
                    "similarity": 0.0,  # ç½‘ç»œæœç´¢ç»“æœæ²¡æœ‰ç›¸ä¼¼åº¦
                })
            
            # ä¿å­˜ç½‘ç»œæœç´¢ç»“æœ
            query_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self._save_query_result(question, "web_search", {
                "results_count": len(web_results),
                "results": web_results[:5]
            }, query_timestamp)
            
            log.info("âœ… ç½‘ç»œæœç´¢ç»“æœå·²åˆå¹¶åˆ°ç­”æ¡ˆä¸­")
            
            return {
                "success": True,
                "answer": merged_answer.strip(),
                "sources": merged_sources,
                "has_web_search": True,  # æ ‡è®°ä½¿ç”¨äº†ç½‘ç»œæœç´¢
                "suggest_web_search": False,  # å·²ç»ä½¿ç”¨äº†ï¼Œä¸å†å»ºè®®
                "max_similarity": max([s.get("similarity", 0) for s in kb_sources]) if kb_sources else 0.0,
            }
            
        except Exception as e:
            log.error(f"ç½‘ç»œæœç´¢å’Œåˆå¹¶å¤±è´¥: {e}")
            # å¦‚æœç½‘ç»œæœç´¢å¤±è´¥ï¼Œè¿”å›çŸ¥è¯†åº“ç»“æœ
            return kb_result
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        è·å–å‘é‡å­˜å‚¨ä¿¡æ¯ã€‚

        Returns:
            é›†åˆä¿¡æ¯
        """
        try:
            info = self.rag_engine.vector_store.get_collection_info()
            return {
                "success": True,
                "info": info,
            }
        except Exception as e:
            log.error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {
                "success": False,
                "info": {},
            }

